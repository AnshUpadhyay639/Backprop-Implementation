{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuwNa2vEiZ6O168GqZ+sGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshUpadhyay639/Backprop-Implementation/blob/main/Language_Model_with_Custom_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxmxr5hDxOVx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sBBm9BYyZP9",
        "outputId": "c69edce2-b2ef-467f-e6a9-c32951488e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VSISRJjycii",
        "outputId": "69fb43e2-143d-4b2b-dacf-b67d5ab393e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gXGVLNAylFE",
        "outputId": "b4196307-9067-47ad-a0de-fd9e1482b870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing our manualy calculated gradients to PyTorch gradients.\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item() # Exactly Similar.\n",
        "  app = torch.allclose(dt, t.grad) # Approx Similiar.\n",
        "  maxdiff = (dt - t.grad).abs().max().item() # Value with the Highest difference.\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "lh2GGFDLyqtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd), generator=g) # Vocab_size = No. of Characters = 27 with each char represented in 10 features.\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEbRo29jyy_7",
        "outputId": "9c1f4372-ba11-4f12-9f13-462d76473e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # fetch random 32 indexes.\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y (First Batch).\n",
        "Xb, Yb"
      ],
      "metadata": {
        "id": "DYvtw7tvy3Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41de5bd3-9506-404e-9912-ca1a40b2f58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1,  1,  4],\n",
              "         [18, 14,  1],\n",
              "         [11,  5,  9],\n",
              "         [ 0,  0,  1],\n",
              "         [12, 15, 14],\n",
              "         [ 0, 17,  1],\n",
              "         [ 0,  0, 13],\n",
              "         [13, 13,  1],\n",
              "         [ 8, 25, 12],\n",
              "         [ 0,  0, 26],\n",
              "         [22, 15, 14],\n",
              "         [19, 13,  9],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  4,  5],\n",
              "         [ 5, 14,  9],\n",
              "         [18,  5,  5],\n",
              "         [ 0,  4,  1],\n",
              "         [ 1, 18,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  5, 12],\n",
              "         [ 0, 10,  1],\n",
              "         [ 9, 14,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0, 18],\n",
              "         [20,  5,  1],\n",
              "         [ 0, 11, 15],\n",
              "         [ 0,  0,  7],\n",
              "         [ 0, 18,  5],\n",
              "         [26,  5, 18],\n",
              "         [ 0,  0, 14],\n",
              "         [ 3,  5, 14],\n",
              "         [ 0, 18, 15]]),\n",
              " tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
              "         26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "emb = C[Xb] # embed the characters into vectors [Feature Embedding: Further Represent those 3 features as 10].\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors OR nn.Flatten().[Normalize for Linear layer input = (32,3*10)].\n",
        "# Linear layer 1:\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer (input logits = hprebn(n=32,features)):\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True) # Summing all the rows i.e(sum of all the 32 samples) to make the shape (1,features) and then dividing it by n=32 gives the current batch mean.\n",
        "bndiff = hprebn - bnmeani # Centering the data points around the Origin (0,0) instead of the mean. So now our Logits will be close to zero.[New mean = 0 but std() remains the same].\n",
        "bndiff2 = bndiff**2 # Defines the Numerator in Variance Formula.\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) [Variance Calculation].\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5 #[Inverse Variance : values with low variance (higher precision) are assigned higher weights, and values with high variance (lower precision) are assigned lower weights.][This ensures that data points with higher precision (smaller variance) contribute more to the result].\n",
        "bnraw = bndiff * bnvar_inv # Dividing the centered data with the std (or mul with inv_std {same thing}) we make the new std = 1.[Now the Data points are evenly Scaled and Normalized].\n",
        "hpreact = bngain * bnraw + bnbias #The bngain is the gamma parameter which scales the normalized data, and bnbias is the beta parameter which shifts it. This allows the network to learn the optimal mean and variance for the normalized inputs.\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values # Find the highest value amongst all the Logits.\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True) # Summation along the rows is being performed.\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...[Here we are normalizing the 'counts_sum'].\n",
        "probs = counts * counts_sum_inv #[counts_sum_inv is a Column Tensor so Pytorch replicates this column 27 times to match the other matrix].\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean() # loss for 1 batch with \"n\" samples = -1/n*[logprobs(i,yi)].sum() ; dloss/dlogprobs = -1/n * 1[== one-hot(Yb) for all the 32 samples].\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik(as far as ik) there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad() # So that we can crossverify with PyTorch's Answers.\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr1pRbtqy7XZ",
        "outputId": "e190d0e9-f634-4bd6-8056-ebbd843a8a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3354, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL77Xx4r3mdA",
        "outputId": "9a908559-ec11-441d-cb0a-b10d4f348237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually.\n",
        "# backpropagating through exactly all of the variables .\n",
        "# as they are defined in the forward pass above, one by one.\n",
        "# Note: The Gradients || Derrivatives has always the same shape as the OG matrixes.[Common Sense].\n",
        "# Start with the Node before root = loss.grad = 1 [Final Function in DAG].\n",
        "dlogprobs = torch.zeros_like(logprobs) # For Batchsize = n [dlogprobs.shape = (32,27)][We making this matrix as we have to store grads of samples other than those 32 whose grads are 0 for this batch as they dont contribute to the Loss()].\n",
        "dlogprobs[range(n), Yb] = -1.0/n # Now store the gradient for all predicted samples on indexes (given by Yb) in a single batch.[Here out.grad = 1].\n",
        "dprobs = (1.0 / probs) * dlogprobs # [local derrivative/gradient * global gradient(out.grad)][Here we are Boosting the grads of labels with low probs].\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # Here in the probs eqn ,2 operations are performed: Multiplication[Local grad * out.grad] && Replication[ As a single Column Vector will be converted to a row vector during BroadCasting and is then duplicated (used multiple times) to adjust with the shape of the other matrix; We sum the gradients when they are being reused; So as each row is being reused so we are using \".sum(dim=1)\"\" (sums up all the columns) to sum up their grads too].\n",
        "dcounts = counts_sum_inv * dprobs # [Local grad * out.grad][No summation required as there will be BroadCasting w.r.t \"counts\"][Summation will be in counts_sum_inv matrix and not in Counts].\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv #[Local.grad * out.grad].\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum # [[Local.grad * out.grad] where Local.grad is calculated by converting (32,1) matrix to a (32,27); NOW imagine a column tensor 1st element b1(from 32,1 matrix) which is formed after adding a1 + a2 + a3 ; and we already know b1.grad So db1/da1 = 1 same goes for a2 and a3 ; so grads of a1 , a2, a3 == 1 for a single row ; similarly for all the rows the respective grads will be 1][\"+=\" used as \"counts\" is used more than once].\n",
        "dnorm_logits = counts * dcounts #[Local.grad * out.grad where Local.grad = normlogits.exp() == counts only (see above)].\n",
        "dlogits = dnorm_logits.clone() # During '+' or '-' operation the grads are directly transfered but logits have a \"+\" sign so copied grads will be positive][.clone() is an Alt].\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # [Logit_maxes has \"-\" sign so -ve logits will be transfered][Replication: Summation of all the Columns will be performed].\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # [Alternative of what we did to calculate 'dlogprobs'][out.grad = dlogitmaxes].\n",
        "dh = dlogits @ W2.T # [Hint: You can see this if u look at the shapes of root eqn = logits][Derrivation in my Notebook].\n",
        "dW2 = h.T @ dlogits # Same as Above.\n",
        "db2 = dlogits.sum(0) #[Due to \"+\" operation dlogits.grad flows into this][Replication : .sum(0) up all the rows][Derrivation in Notebook].\n",
        "dhpreact = (1.0 - h**2) * dh #[Local.grad * out.grad].\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact # We didnt sum here as bnraw matrix has already a same shape as hpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv #[Local.grad * out.grad].\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar #[Local.grad * out.grad].[Matrix of all 1s because due to summation grads are all 1s. Shape is same as bndiff2].\n",
        "dbndiff += (2*bndiff) * dbndiff2 #[Local.grad * out.grad].\n",
        "dhprebn = dbndiff.clone() # Gradient flows/copies due to \"+\" sign.\n",
        "dbnmeani = (-dbndiff).sum(0) # Gradient flows/copies due to \"-\" sign.[Replication: .sum(0)].[This gradient will return False during comparison below as it is not completely made up of leaf nodes; Its 1 Node dependencies goes further].\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani) # [Converse Rule: BroadCasting happens in backward pass as .sum() is used in the Forward Pass].\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape) # Re-represent the Original View of what shape it was before i.e [Make demb.shape = emb.shape but also include out.shape].\n",
        "dC = torch.zeros_like(C) # First replicate an empty C.\n",
        "for k in range(Xb.shape[0]): # Not iterate over Xb rows and cols and fill dC.\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j] # Pluck out each value.\n",
        "    dC[ix] += demb[k,j] # Place it in right position of dC using out.matrix[Use \"+=\" as there can be multiple 'ix' with same value so add their grads].\n",
        "\n",
        "# Verifying our grads with PyTorch.grads:\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbHf-dmy-6d",
        "outputId": "1afe2bc0-c3bd-4bee-ecb5-5fe4e00f0b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.259629011154175e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "\n",
        "  # logit_maxes = logits.max(1, keepdim=True).values\n",
        "  # norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "  # counts = norm_logits.exp()\n",
        "  # counts_sum = counts.sum(1, keepdims=True)\n",
        "  # counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "  # probs = counts * counts_sum_inv\n",
        "  # logprobs = probs.log()\n",
        "  # loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb) # Now we can calculate 'dlogits' directly. [Function of Cross_Entropy() uses SoftMAX for a single sample it does: 1. Perform .exp() ||||2. Divid by .sum() of all exps to make them in range (0,1) Softmax part done |||| 3. Perform -log (O/p of Softmax)].\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item()) # Now we have to find \"dL/dlogiti\" for back prop grads."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsltyzxfzEwW",
        "outputId": "45db0da0-842d-452e-b2f2-211a65a510bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.335381031036377 diff: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "# See in Notebook where the below came from:\n",
        "dlogits = F.softmax(logits, 1) # Here dim = 1 means we want to do the softmax for a single data point(summing all the single row elements || Summation ALONG the rows) that is adding the columns (divide each exp() of a row divide by sum of columns.exp()).\n",
        "dlogits[range(n), Yb] -= 1 # If Logiti == Yb we substract that by 1.(To make them move more towards -ve gradient direction)[Look derrivation in Notebook][This is dlogits for a single sample].\n",
        "dlogits /= n # To get the Average dlogits of a batch you divide by batchsize.\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 7e-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isegztHCzKM7",
        "outputId": "548dbee8-9e0b-424c-ade8-e9752cbf195e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing \"dlogits\" of Shape(32,27):\n",
        "plt.figure(figsize=(4, 4)) # Black spots are the position of corrected idx where we substracted 1 so that they move more closely towards -ve grad direction.\n",
        "plt.imshow(dlogits.detach(), cmap='gray') # These Black sq are pulled up by a force of dlogits[blacki] which is basically the grad and vice versa white is pulled down."
      ],
      "metadata": {
        "id": "xTopk5MmzNk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "195916c9-1a0c-464e-b571-ce3ef3af2d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7de5516b8b50>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go.\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "  # bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "  # bndiff = hprebn - bnmeani\n",
        "  # bndiff2 = bndiff**2\n",
        "  # bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "  # bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "  # bnraw = bndiff * bnvar_inv\n",
        "  # hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmgnTYD0vEmQ",
        "outputId": "ff42bcbb-bf47-476f-ecd5-4c6870ade7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)) # Backprop expression fora single batch of 32 for BatchNorm1D [Refer to Notebook for derrivation of a single sample].\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nv5ndBNxLIk",
        "outputId": "b64e00f8-953b-4a92-bf9d-62f33ed6bcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2p4vmoEfxPX_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}